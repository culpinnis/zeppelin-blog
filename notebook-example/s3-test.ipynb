{
  "metadata": {
    "name": "s3-test",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark-s3.pyspark\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import Row\nimport pyspark.sql.functions as f\n\n\nprint(\"################################################\")\nprint(\"Test 1: create RDD\")\n\nrdd \u003d sc.parallelize(range(100000000))\nprint(rdd.sum())\nprint(\"################################################\")\nprint(\"Test 2: read singel file from s3 (city_value_histogram)\")\n  \n# read simple csv\ndf\u003d(spark\n    .read\n    .option(\"delimiter\",\";\")\n    .csv(\"s3a://import/city/city_value_histogram.csv\",inferSchema\u003dTrue,header\u003dTrue)\n    )\ndf.show()\nprint(\"################################################\")\nprint(\"Test 3: read large file from s3 (bike-sharing-dataset/raw/hour.csv)\")\n# read large csv\ndf_bikes1\u003d(spark\n    .read\n    .option(\"delimiter\",\",\")\n    .csv(\"s3a://import/bike-sharing-dataset/raw/hour.csv\",inferSchema\u003dTrue,header\u003dTrue)\n    )\n\ndf_bikes2\u003d(df_bikes1\n)\ndf_bikes2.show()\ndf_bikes2.printSchema()\ndf_bikes2.select(\"mnth\").distinct().show()\n\n\n"
    }
  ]
}